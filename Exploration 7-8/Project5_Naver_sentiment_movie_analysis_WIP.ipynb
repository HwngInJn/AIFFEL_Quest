{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 영화리뷰 감성분석 도전하기 \n",
    "#### 코드 읽어보면서 따라가기 (https://github.com/rurube/Aiffel_Online5/blob/master/Exploration4/naver_movie.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 라이브러리 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.3\n",
      "0.5.2\n",
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import konlpy\n",
    "import gensim\n",
    "\n",
    "print(pandas.__version__)\n",
    "print(konlpy.__version__)\n",
    "print(gensim.__version__)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 데이터 준비와 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        150000 non-null  int64 \n",
      " 1   document  149995 non-null  object\n",
      " 2   label     150000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 데이터로더 구성\n",
    "* nsmc 데이터셋은 전혀 가공되지 않은 텍스트 파일로 이루어져 있음\n",
    "* 이것을 imdb.data_loader()와 동일하게 동작하는 자신만의 data_loader를 만들어 보는 것으로 시작\n",
    "* data_loader 안에서 수행할 작업:\n",
    "######\n",
    "###### 데이터의 중복 제거\n",
    "###### NaN 결측치 제거\n",
    "###### 한국어 토크나이저로 토큰화\n",
    "###### 불용어(Stopwords) 제거\n",
    "###### 사전word_to_index 구성\n",
    "###### 텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "###### X_train, y_train, X_test, y_test, word_to_index 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)  # 학습 데이터 중복 제거\n",
    "    train_data = train_data.dropna(how = 'any')  # 학습 데이터 NaN 결측치 제거\n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # 테스트 데이터 중복 제거\n",
    "    test_data = test_data.dropna(how = 'any')  # 학습 데이터 NaN 결측치 제거\n",
    "    \n",
    "    X_train = [] #X_train 리스트 \n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) #문장 단위 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_x)\n",
    "\n",
    "    X_test = [] #X_test 리스트\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) #문장 단위 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_x)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(num_words-4)\n",
    "    vocab = ['<PAD>','<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "        \n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "        \n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 모델 구성을 위한 데이터 분석 및 가공\n",
    "* 데이터셋 내 문장 길이 분포\n",
    "* 적절한 최대 문장 길이 지정\n",
    "* keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len:  116\n",
      "mean len:  15.98170773419436\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('max len: ', max(len(review) for review in X_train)) # 데이터셋 내 최대 문장 길이\n",
    "print('mean len: ', sum(map(len, X_train))/len(X_train)) # 데이터셋 문장 길이 평균\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.show() # 문장 길이 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    count = 0\n",
    "    for sentence in nested_list:\n",
    "        if(len(sentence) <= max_len):\n",
    "            count += 1\n",
    "    print(f'최대 길이: {max_len}, 커버 가능한 샘플 비율: {(count/len(nested_list))*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이: 50, 커버 가능한 샘플 비율: 96.03439547960761\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(50, X_train) # 최대 길이 50일 때 96퍼센트의 샘플 커버 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# 최대 문장 길이 50으로 결정\n",
    "# 패딩 추가\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='pre',maxlen=50)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='pre',maxlen=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 모델 구성 및 validation set 구성 (3가지 이상)\n",
    "* (1) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 30)                9720      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 509,751\n",
      "Trainable params: 509,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델 준비\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "vocab_size = 10000 \n",
    "word_vector_dim = 50\n",
    "hidden_units = 30\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, word_vector_dim))\n",
    "model_lstm.add(LSTM(hidden_units))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) # val_loss가 4번 떨어지면 학습 멈춤\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2) 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1D CNN 모델 준비\n",
    "from tensorflow.keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "vocab_size = 10000\n",
    "word_vector_dim_cnn = 16\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(vocab_size, word_vector_dim_cnn))\n",
    "model_cnn.add(Conv1D(16, 7, activation='relu'))\n",
    "model_cnn.add(MaxPooling1D(5))\n",
    "model_cnn.add(Conv1D(16, 7, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(8, activation='relu'))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, shuffle=True,\\\n",
    "                                                 stratify=y_train, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) 모델 훈련 개시\n",
    "* (1) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "172/172 [==============================] - 33s 10ms/step - loss: 0.4866 - accuracy: 0.7762 - val_loss: 0.3787 - val_accuracy: 0.8350\n",
      "Epoch 2/15\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.3345 - accuracy: 0.8582 - val_loss: 0.3508 - val_accuracy: 0.8478\n",
      "Epoch 3/15\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.3011 - accuracy: 0.8732 - val_loss: 0.3514 - val_accuracy: 0.8461\n",
      "Epoch 4/15\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.2796 - accuracy: 0.8840 - val_loss: 0.3514 - val_accuracy: 0.8488\n",
      "Epoch 5/15\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.2600 - accuracy: 0.8936 - val_loss: 0.3625 - val_accuracy: 0.8486\n",
      "Epoch 6/15\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.2434 - accuracy: 0.9010 - val_loss: 0.3696 - val_accuracy: 0.8471\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(X_train, y_train, epochs=15, callbacks=es, batch_size=512,\\\n",
    "                    validation_data=(X_val, y_val), verbose=1) # LSTM 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.3709 - accuracy: 0.8467\n",
      "[0.37085679173469543, 0.8466953039169312]\n"
     ]
    }
   ],
   "source": [
    "result_lstm = model_lstm.evaluate(X_test, y_test, verbose=2)\n",
    "print(result_lstm) # LSTM 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2) 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "172/172 [==============================] - 21s 10ms/step - loss: 0.6134 - accuracy: 0.6452 - val_loss: 0.4994 - val_accuracy: 0.7484\n",
      "Epoch 2/15\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4564 - accuracy: 0.7745 - val_loss: 0.4702 - val_accuracy: 0.7656\n",
      "Epoch 3/15\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4199 - accuracy: 0.7944 - val_loss: 0.4697 - val_accuracy: 0.7656\n",
      "Epoch 4/15\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3974 - accuracy: 0.8068 - val_loss: 0.4768 - val_accuracy: 0.7644\n",
      "Epoch 5/15\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3763 - accuracy: 0.8186 - val_loss: 0.4874 - val_accuracy: 0.7647\n",
      "Epoch 6/15\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3526 - accuracy: 0.8304 - val_loss: 0.5025 - val_accuracy: 0.7624\n",
      "Epoch 7/15\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3272 - accuracy: 0.8439 - val_loss: 0.5264 - val_accuracy: 0.7599\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "history_cnn = model_cnn.fit(X_train, y_train, epochs=15, callbacks=es, batch_size=512, \\\n",
    "                        validation_data=(X_val, y_val), verbose=1) # 1D CNN 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.5223 - accuracy: 0.7571\n",
      "[0.5222887992858887, 0.7570844292640686]\n"
     ]
    }
   ],
   "source": [
    "result_cnn = model_cnn.evaluate(X_test, y_test, verbose=2)\n",
    "print(result_cnn) #1D CNN 결과"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
