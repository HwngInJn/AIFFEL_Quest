{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Quest Project\n",
    "## Project 1. Pneumonia diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가문항 (상세기준)\n",
    "#### 1. 의료영상을 처리하는 CNN 기반 딥러닝 모델이 잘 구현되었다.\t\n",
    "* (모델 학습이 안정적으로 수렴하는 것을 시각화를 통해 확인하였다.)\n",
    "#### 2. 데이터 준비, 모델구성 등의 과정의 다양한 실험이 체계적으로 수행되었다.\t\n",
    "* (regularization, augmentation 등의 기법의 사용 여부에 따른 모델 성능 측정이 ablation study 형태로 체계적으로 수행되었다.)\n",
    "#### 3. 실습코드를 잘 개선하여 폐렴 검출 정확도가 추가로 향상되었다.\t\n",
    "* (Accuracy 기준 85%에 도달하였다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 디렉토리 생성\n",
    "$ mkdir -p ~/aiffel/chest_xray\n",
    "$ ln -s ~/data/ ~/aiffel/chest_xray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. 실험환경 Set-up\n",
    "#### Batch size, Epoch 등 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용할 패키지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import random, math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from tf import keras\n",
    "from tf.keras import layers, backend, regularizers, initializers, models\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 필요한 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드할 때 빠르게 로드할 수 있도록하는 설정 변수\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# X-RAY 이미지 사이즈 변수\n",
    "IMAGE_SIZE = [180, 180]\n",
    "\n",
    "# 데이터 경로 변수\n",
    "ROOT_PATH = os.path.join(os.getenv('HOME'), 'aiffel')\n",
    "TRAIN_PATH = ROOT_PATH + '/chest_xray/data/train/*/*' # *은 모든 디렉토리와 파일을 의미합니다.\n",
    "VAL_PATH = ROOT_PATH + '/chest_xray/data/val/*/*'\n",
    "TEST_PATH = ROOT_PATH + '/chest_xray/data/test/*/*'\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "EPOCHS = 20\n",
    "\n",
    "print(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 준비하기\n",
    "#### 원본 데이터를 가져와서 전처리 및 배치 구성을 진행\n",
    "#### 데이터 부족을 극복하기 위해 augmentation 기법 고려\n",
    "* 의료 영상인 경우, 일반적인 이미지 처리에서 사용하는 augmentation들이 항상 도움이 된다고 말할 수 없음\n",
    "* X-RAY 같은 의료 영상의 특성상, 육안으로도 구분하기 어려운 미묘한 차이에 더해진 노이즈 등 부수효과가 오히려 방해를 줄 수도 있음\n",
    "###### 좌우 반전, 상하 반전 정도의 augmentation만 도입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 개수 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = tf.io.gfile.glob(TRAIN_PATH)\n",
    "test_filenames = tf.io.gfile.glob(TEST_PATH)\n",
    "val_filenames = tf.io.gfile.glob(VAL_PATH)\n",
    "\n",
    "print(len(train_filenames))\n",
    "print(len(test_filenames))\n",
    "print(len(val_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train : val 합친 뒤 다시 분할\n",
    "* 정상 이미지 수와 폐렴 이미지 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : val 합친 뒤 다시 분할\n",
    "# train 데이터와 validation 데이터를 모두 filenames에 담습니다\n",
    "filenames = tf.io.gfile.glob(TRAIN_PATH)\n",
    "filenames.extend(tf.io.gfile.glob(VAL_PATH))\n",
    "\n",
    "# 모아진 filenames를 8:2로 나눕니다\n",
    "train_size = math.floor(len(filenames)*0.8)\n",
    "random.seed(8)\n",
    "random.shuffle(filenames)\n",
    "train_filenames = filenames[:train_size]\n",
    "val_filenames = filenames[train_size:]\n",
    "\n",
    "print(len(train_filenames))\n",
    "print(len(val_filenames))\n",
    "\n",
    "# 정상 이미지 수와 폐렴 이미지 수 확인\n",
    "COUNT_NORMAL = len([filename for filename in train_filenames if \"NORMAL\" in filename])\n",
    "print(f\"Normal images count in training set: {COUNT_NORMAL}\")\n",
    "\n",
    "\n",
    "COUNT_PNEUMONIA = len([filename for filename in train_filenames if \"PNEUMONIA\" in filename])\n",
    "print(f\"Pneumonia images count in training set: {COUNT_PNEUMONIA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.data 인스턴스를 생성\n",
    "###### tf.data는 tensorflow에서 학습시킬 때, mini-batch로 작업할 수 있도록 해 줌\n",
    "* Train 데이터셋, validation 데이터셋 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data 인스턴스를 생성\n",
    "train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\n",
    "val_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\n",
    "\n",
    "# Train 데이터셋, validation 데이터셋 개수 확인\n",
    "TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\n",
    "print(f\"Training images count: {TRAIN_IMG_COUNT}\")\n",
    "\n",
    "VAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\n",
    "print(f\"Validating images count: {VAL_IMG_COUNT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 현재 이미지에는 라벨 데이터가 따로 없음\n",
    "* 파일 경로에 'NORMAL'이나 'PNEUMONIA'가 포함되어 있기 때문에 이를 이용해서 라벨 데이터를 만들어 주는 함수를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = np.array([str(tf.strings.split(item, os.path.sep)[-1].numpy())[2:-1]\n",
    "                        for item in tf.io.gfile.glob(str(ROOT_PATH + \"/chest_xray/train/*\"))])\n",
    "print(CLASS_NAMES)\n",
    "\n",
    "# 파일 경로의 끝에서 두번째 부분을 확인하면 양성과 음성을 구분할 수 있습니다\n",
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    return parts[-2] == \"PNEUMONIA\"   # 폐렴이면 양성(True), 노말이면 음성(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이미지 데이터는 현실적으로 사이즈가 제각각일 가능성이 높음\n",
    "* 이미지의 사이즈를 통일 시키고 GPU 메모리를 더욱 효율적으로 사용하기 위해 이미지 사이즈를 축소\n",
    "###### decode_img 함수와 process_path 함수 생성\n",
    "###### process_path 함수에서 decode_img 함수를 이용해서 이미지의 데이터 타입을 float으로 바꾸고 사이즈를 변경\n",
    "###### get_label을 이용해서 라벨 값을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 알맞은 형식으로 바꿉니다.\n",
    "def decode_img(img):\n",
    "    img = tf.image.decode_jpeg(img, channels=3) # 이미지를 uint8 tensor로 수정\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32) # float32 타입으로 수정\n",
    "    img = tf.image.resize(img, IMAGE_SIZE) # 이미지 사이즈를 IMAGE_SIZE로 수정\n",
    "    return img\n",
    "\n",
    "# 이미지 파일의 경로를 입력하면 이미지와 라벨을 읽어옵니다.\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path) # 라벨 검출\n",
    "    img = tf.io.read_file(file_path) # 이미지 읽기\n",
    "    img = decode_img(img) # 이미지를 알맞은 형식으로 수정\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train 데이터 셋과 validation 데이터 셋을 생성\n",
    "###### num_parallel_calls 파라미터에 위에서 할당한 AUTOTUNE변수를 이용하면 더욱 빠르게 데이터를 처리해 즐 수 있음\n",
    "* 이미지가 잘 리사이즈 되었는지, 그리고 라벨이 잘 들어가 있는지 확인\n",
    "###### train_ds.take(1)은 하나의 데이터만 가져온다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train 데이터 셋과 validation 데이터 셋을 생성\n",
    "train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# 이미지가 잘 리사이즈 되었는지, 그리고 라벨이 잘 들어가 있는지 확인\n",
    "for image, label in train_ds.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train과 validation 데이터셋을 만든 것처럼 test 데이터셋도 생성\n",
    "* 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list_ds = tf.data.Dataset.list_files(TEST_PATH)\n",
    "TEST_IMAGE_COUNT = tf.data.experimental.cardinality(test_list_ds).numpy()\n",
    "test_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "\n",
    "print(TEST_IMAGE_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensorflow에서는 tf.data 파이프라인을 사용해서 학습 데이터를 효율적으로 사용할 수 있도록 해줌\n",
    "###### tf.data 파이프라인을 이용하여 prepare_for_training() 함수를 정의해서 데이터를 변환\n",
    "###### shuffle()을 사용하면 고정 크기 버퍼를 유지하고 해당 버퍼에서 무작위로 균일하게 다음 요소를 선택\n",
    "###### repeat()를 사용하면 epoch를 진행하면서 여러 번 데이터셋을 불러오게 되는데, 이때 repeat()를 사용한 데이터셋의 경우 여러 번 데이터셋을 사용할 수 있게 해줌\n",
    "###### (100개의 데이터를 10번 반복하면 1000개의 데이터가 필요하게 되는데, repeat()를 사용하면 자동으로 데이터를 맞춰줌)\n",
    "###### batch()를 사용하면 BATCH_SIZE에서 정한 만큼의 배치로 주어짐\n",
    "###### (100개의 데이터를 10개의 배치로 나누게 되면 각 배치에는 10개의 데이터로 나뉘게 됨)\n",
    "###### prefetch()를 사용하면 학습 데이터를 나눠서 읽어오기 때문에, 첫 번째 데이터를 GPU에서 학습하는 동안 두 번째 데이터를 CPU에서 준비할 수 있어 리소스의 유휴 상태를 줄일 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image,label):\n",
    "    image = tf.image.random_flip_left_right(image)  # 랜덤하게 좌우를 반전\n",
    "    image = tf.image.random_flip_\n",
    "    return image,label\n",
    "\n",
    "def prepare_for_training(ds, shuffle_buffer_size=1000):\n",
    "    ds = ds.map(\n",
    "            augment,       \n",
    "            num_parallel_calls=2\n",
    "        )# augment 함수 적용\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = prepare_for_training(train_ds)\n",
    "val_ds = prepare_for_training(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 시각화\n",
    "#### 학습용 데이터를 시각화해서 확인\n",
    "#### show_batch() 함수를 통해 augmentation의 좌우 반전 등이 제대로 처리되었는지 확인\n",
    "* 데이터를 보기 위해 먼저, train에 있는 batch 중 첫 번째 배치를 추출\n",
    "*  추출된 배치를 image와 label 데이터 셋으로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 배치를 입력하면 여러장의 이미지를 보여줍니다.\n",
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for n in range(BATCH_SIZE):\n",
    "        ax = plt.subplot(4,math.ceil(BATCH_SIZE/4),n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        if label_batch[n]:\n",
    "            plt.title(\"PNEUMONIA\")\n",
    "        else:\n",
    "            plt.title(\"NORMAL\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "image_batch, label_batch = next(iter(train_ds))\n",
    "show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. ResNet-18 구현 ()\n",
    "#### 의료 영상 판독을 위해 실습에서 구현했던 model에서 다양한 것들을 바꾸어 가며 실험\n",
    "* Convolution filter, 채널 개수, activation, 모델 구조 등\n",
    "#### ResNet-18: ResNet의 가장 작은 버전\n",
    "* ResNet의 특징: Residual Connection으로 학습된 정보가 데이터 처리과정에서 손실되는 것을 방지\n",
    "* Residual block 구성\n",
    "#### ResNet에서의 weight layer:\n",
    "* 3x3 CNN\n",
    "* BatchNormalization\n",
    "\n",
    "### 해당 부분 구현 실패로 노드의 CNN 적용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convolution block 생성\n",
    "###### Convolution을 두 번 진행\n",
    "###### Batch Normalization을 통해서 Gradient vanishing, Gradient Exploding을 해결\n",
    "###### Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(filters):\n",
    "    block = tf.keras.Sequential([\n",
    "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D()\n",
    "    ])\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dense Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(units, dropout_rate):\n",
    "    block = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 해당 모델은 약간의 수정을 거쳐 만들어진 CNN 모델 (전형적인 CNN 모델과는 약간 다름)\n",
    "* Batch Normalization과 Dropout이라는 두 가지 regularization 기법이 동시에 사용\n",
    "###### 일반적으로 이런 방법은 잘 사용되지 않거나, 금기시됨 \n",
    "###### variance shift를 억제하는 Batch Normalization과 이를 유발하는 Dropout을 동시에 사용하는 것이 어울리지 않음 (https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.pdf)\n",
    "###### 예외적으로 동시에 사용하는 것이 성능 향상에 도움을 주는 경우가 있음 (https://arxiv.org/pdf/1905.05928.pdf)\n",
    "* 두 가지를 함께 사용하는 이 모델이 성능 향상에 도움이 될지 여부도 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        \n",
    "        conv_block(32),\n",
    "        conv_block(64),\n",
    "        \n",
    "        conv_block(128),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        conv_block(256),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        dense_block(512, 0.7),\n",
    "        dense_block(128, 0.5),\n",
    "        dense_block(64, 0.3),\n",
    "        \n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. 데이터 imbalance 처리\n",
    "실습 코드에서 데이터의 imbalance 문제에 대처하기 위해 데이터 비율로 나누어진 class_weight를 설정해 주었습니다. 만약 이러한 처리를 생략한다면 어떻게 될까요? 또 recall을 강조하기 위해 폐렴 데이터를 잘 맞추는 것을 더 강화하는 효과를 만들어낼 수는 없을까요?\n",
    "\n",
    "* Imbalance: 한 라벨이 너무 많은 경우, 데이터를 학습할 때 imbalance한 데이터의 경우 학습 효과가 좋지 않을 수 있음\n",
    "* Weight balancing: training set의 각 데이터에서 loss를 계산할 때 특정 클래스의 데이터에 더 큰 loss 값을 갖도록 가중치를 부여하는 방법, imbalance 해결에 사용\n",
    "###### Keras는 model.fit()을 호출할 때 파라미터로 넘기는 class_weight 에 이러한 클래스별 가중치를 세팅할 수 있도록 지원\n",
    "###### weight_for_0: 'Normal' 이미지에 사용할 weight\n",
    "###### weight_for_1: 'Pneumonia' 이미지에 사용할 weight\n",
    "###### weight들은 'Normal'과 'Pneumonia' 전체 데이터 건수에 반비례하도록 설정\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_0 = (1 / COUNT_NORMAL)*(TRAIN_IMG_COUNT)/2.0 \n",
    "weight_for_1 = (1 / COUNT_PNEUMONIA)*(TRAIN_IMG_COUNT)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for NORMAL: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for PNEUMONIA: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6. 모델 훈련\n",
    "loss 함수를 변경하기는 어렵겠지만, optimizer나 learning rate 등의 변화를 고려해볼 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model = build_model()\n",
    "\n",
    "    METRICS = [\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=METRICS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델을 fit 하기 \n",
    "각 파라미터에 위에서 선언했었던 변수, 데이터 셋을 가져와서 각각에 맞게 넣어줌\n",
    "###### 학습하는데 긴 시간 소요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        steps_per_epoch=TRAIN_IMG_COUNT // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        validation_steps=VAL_IMG_COUNT // BATCH_SIZE,\n",
    "        class_weight=class_weight,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7. 결과 확인과 시각화\n",
    "테스트 데이터로 훈련된 모델을 평가해 봅시다. 우선은 accuracy를 고려해야겠지만 의료 영상 모델의 특성상 recall도 중요합니다. 훈련과정의 history 그래프를 시각화해 보고, 학습 진행 양상을 면밀히 분석해 보는 것도 잊지 않도록 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(20, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['precision', 'recall', 'accuracy', 'loss']):\n",
    "    ax[i].plot(history.history[met])\n",
    "    ax[i].plot(history.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 테스트 데이터로 모델 평가\n",
    "###### 모델 평가를 위해 loss, accuracy, precision, recall 값을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, precision, recall = model.evaluate(test_ds)\n",
    "print(f'Loss: {loss},\\nAccuracy: {accuracy},\\nPrecision: {precision},\\nRecall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고\n",
    "* Resnet 구현 시도에 시간이 너무 소요되었다. 기존 Resnet50을 18로 변환하는 과정에서 문제가 계속 발생했는데 어떤 부분이 문제인지 모르겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os, re\n",
    "import random, math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from keras import layers, backend, regularizers, initializers, models\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "\n",
    "# block 안에 반복적으로 활용되는 L2 regularizer를 선언해 줍니다.\n",
    "def _gen_l2_regularizer(use_l2_regularizer=True, l2_weight_decay=1e-4):\n",
    "  return regularizers.l2(l2_weight_decay) if use_l2_regularizer else None\n",
    "\n",
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2),\n",
    "               use_l2_regularizer=True,\n",
    "               BATCH_NORM_DECAY=0.9,\n",
    "               BATCH_NORM_EPSILON=1e-5):\n",
    "    \n",
    "    filters1, filters2 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters1, (3, 3),\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '2a')(\n",
    "            input_tensor)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=BATCH_NORM_DECAY,\n",
    "        epsilon=BATCH_NORM_EPSILON,\n",
    "        name=bn_name_base + '2a')(\n",
    "            x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters2,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '2b')(\n",
    "            x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=BATCH_NORM_DECAY,\n",
    "        epsilon=BATCH_NORM_EPSILON,\n",
    "        name=bn_name_base + '2b')(\n",
    "            x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    shortcut = layers.Conv2D(\n",
    "        filters2, (3, 3),\n",
    "        strides=strides,\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '1')(\n",
    "            input_tensor)\n",
    "    shortcut = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=BATCH_NORM_DECAY,\n",
    "        epsilon=BATCH_NORM_EPSILON,\n",
    "        name=bn_name_base + '1')(\n",
    "            shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Q. identity_block을 가져옵니다.\n",
    "def identity_block(input_tensor,\n",
    "                   kernel_size,\n",
    "                   filters,\n",
    "                   stage,\n",
    "                   block,\n",
    "                   strides=(1, 1),\n",
    "                   use_l2_regularizer=True,\n",
    "                   BATCH_NORM_DECAY=0.9,\n",
    "                   BATCH_NORM_EPSILON=1e-5):\n",
    "\n",
    "    filters1, filters2 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        kernel_size, filters1, \n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '2a')(\n",
    "            input_tensor)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=BATCH_NORM_DECAY,\n",
    "        epsilon=BATCH_NORM_EPSILON,\n",
    "        name=bn_name_base + '2a')(\n",
    "            x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        kernel_size,\n",
    "        filters2,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name=conv_name_base + '2b')(\n",
    "            x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=BATCH_NORM_DECAY,\n",
    "        epsilon=BATCH_NORM_EPSILON,\n",
    "        name=bn_name_base + '2b')(\n",
    "            x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Q. resnet50 함수를 가져옵니다.\n",
    "def resnet18(num_classes,\n",
    "             batch_size=None,\n",
    "             use_l2_regularizer=True,\n",
    "             rescale_inputs=False,\n",
    "             BATCH_NORM_DECAY=0.9,\n",
    "             BATCH_NORM_EPSILON=1e-5):\n",
    "    \n",
    "    input_shape = (32, 32, 3)\n",
    "    img_input = layers.Input(shape=input_shape, batch_size=batch_size)\n",
    "\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        x = layers.Lambda(\n",
    "            lambda x: backend.permute_dimensions(x, (0, 3, 1, 2)),\n",
    "            name='transpose')(\n",
    "            img_input)\n",
    "        bn_axis = 1\n",
    "    else:  # channels_last\n",
    "        x = img_input\n",
    "        bn_axis = 3\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(x)\n",
    "    x = layers.Conv2D(\n",
    "        64, (7, 7),\n",
    "        strides=(2, 2),\n",
    "        padding='valid',\n",
    "        use_bias=False,\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name='conv1')(\n",
    "            x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=BATCH_NORM_DECAY,\n",
    "        epsilon=BATCH_NORM_EPSILON,\n",
    "        name='bn_conv1')(\n",
    "            x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        3, [64, 64],\n",
    "        stage=2,\n",
    "        block='a',\n",
    "        strides=(1, 1),\n",
    "        use_l2_regularizer=use_l2_regularizer)\n",
    "    x = identity_block(\n",
    "        x,\n",
    "        3, [64, 64],\n",
    "        stage=2,\n",
    "        block='b',\n",
    "        use_l2_regularizer=use_l2_regularizer)\n",
    "    \n",
    "    x = conv_block(\n",
    "        x,\n",
    "        3, [128, 128],\n",
    "        stage=3,\n",
    "        block='a',\n",
    "        use_l2_regularizer=use_l2_regularizer)\n",
    "    x = identity_block(\n",
    "        x,\n",
    "        3, [128, 128],\n",
    "        stage=3,\n",
    "        block='b',\n",
    "        use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        3, [256, 256],\n",
    "        stage=4,\n",
    "        block='a',\n",
    "        use_l2_regularizer=use_l2_regularizer)\n",
    "    x = identity_block(\n",
    "        x,\n",
    "        3, [256, 256],\n",
    "        stage=4,\n",
    "        block='b',\n",
    "        use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        3, [512, 512],\n",
    "        stage=5,\n",
    "        block='a',\n",
    "        use_l2_regularizer=use_l2_regularizer)\n",
    "    x = identity_block(\n",
    "        x,\n",
    "        3, [512, 512],\n",
    "        stage=5,\n",
    "        block='b',\n",
    "        use_l2_regularizer=use_l2_regularizer)\n",
    "\n",
    "    rm_axes = [1, 2] if backend.image_data_format() == 'channels_last' else [2, 3]\n",
    "    x = layers.Lambda(lambda x: backend.mean(x, rm_axes), name='reduce_mean')(x)\n",
    "    x = layers.Dense(\n",
    "        num_classes,\n",
    "        kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "        kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        bias_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "        name='fc1000')(\n",
    "            x)\n",
    "\n",
    "    # A softmax that is followed by the model loss must be done cannot be done\n",
    "    # in float16 due to numeric issues. So we pass dtype=float32.\n",
    "    x = layers.Activation('softmax', dtype='float32')(x)\n",
    "\n",
    "    return models.Model(img_input, x, name='resnet18')\n",
    "\n",
    "# 모델 생성 (VGG보다 많은 레이어 수, 10M 정도 작은 모델 크기):\n",
    "model = resnet18(num_classes=100)\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
